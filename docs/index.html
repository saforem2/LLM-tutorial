<!DOCTYPE html>
<html lang="en"><head>
<link href="././favicon.svg" rel="icon" type="image/svg+xml">
<script src="site_libs/quarto-html/tabby.min.js"></script>
<script src="site_libs/quarto-html/popper.min.js"></script>
<script src="site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="site_libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="site_libs/quarto-contrib/roughnotation-0.5.1/rough-notation.iife.js"></script>
<script src="site_libs/quarto-contrib/roughnotation-init-1.0.0/rough.js"></script>
<script src="site_libs/quarto-contrib/iconify-1.0.7/iconify-icon.min.js"></script>
<link href="site_libs/quarto-contrib/academicons-1.9.2/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/academicons-1.9.2/size.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/bootstrap-icons-1.11.1/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/all.css" rel="stylesheet">
<link href="site_libs/quarto-contrib/fontawesome6-0.1.0/latex-fontsize.css" rel="stylesheet"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.3.450">

  <meta name="author" content="Sam Foreman ">
  <meta name="dcterms.date" content="2023-11-30">
  <title>Creating Small(-ish) LLMs</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
    /* CSS for syntax highlighting */
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #383a42;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #383a42; } /* Normal */
    code span.al { color: #95da4c; background-color: #4d1f24; font-weight: bold; } /* Alert */
    code span.an { color: #50a14f; } /* Annotation */
    code span.at { color: #a626a4; } /* Attribute */
    code span.bn { color: #986801; } /* BaseN */
    code span.bu { color: #a626a4; } /* BuiltIn */
    code span.cf { color: #a626a4; } /* ControlFlow */
    code span.ch { color: #50a14f; } /* Char */
    code span.cn { color: #986801; } /* Constant */
    code span.co { color: #a0a1a7; font-style: italic; } /* Comment */
    code span.cv { color: #e45649; font-style: italic; } /* CommentVar */
    code span.do { color: #e45649; } /* Documentation */
    code span.dt { color: #a626a4; } /* DataType */
    code span.dv { color: #986801; } /* DecVal */
    code span.er { color: #f44747; text-decoration: underline; } /* Error */
    code span.ex { color: #4078f2; font-weight: bold; } /* Extension */
    code span.fl { color: #986801; } /* Float */
    code span.fu { color: #4078f2; } /* Function */
    code span.im { color: #50a14f; } /* Import */
    code span.in { color: #c45b00; } /* Information */
    code span.kw { color: #a626a4; } /* Keyword */
    code span.op { color: #a626a4; } /* Operator */
    code span.ot { color: #27ae60; } /* Other */
    code span.pp { color: #a626a4; } /* Preprocessor */
    code span.re { color: #2980b9; background-color: #153042; } /* RegionMarker */
    code span.sc { color: #0184bc; } /* SpecialChar */
    code span.ss { color: #da4453; } /* SpecialString */
    code span.st { color: #50a14f; } /* String */
    code span.va { color: #e45649; } /* Variable */
    code span.vs { color: #da4453; } /* VerbatimString */
    code span.wa { color: #da4453; } /* Warning */
    /* CSS for citations */
    div.csl-bib-body { }
    div.csl-entry {
      clear: both;
    }
    .hanging-indent div.csl-entry {
      margin-left:2em;
      text-indent:-2em;
    }
    div.csl-left-margin {
      min-width:2em;
      float:left;
    }
    div.csl-right-inline {
      margin-left:2em;
      padding-left:1em;
    }
    div.csl-indent {
      margin-left: 2em;
    }  </style>
  <link rel="stylesheet" href="site_libs/revealjs/dist/theme/quarto.css">
  <link rel="stylesheet" href="css/default.css">
  <link rel="stylesheet" href="css/callouts-html.css">
  <script async="" src="https://www.googletagmanager.com/gtag/js?id=G-XVM2Y822Y1"></script>

  <script type="text/javascript">

  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());
  gtag('config', 'G-XVM2Y822Y1', { 'anonymize_ip': true});
  </script>
  <link href="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-titled.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-titled) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-titled.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-titled .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-titled .callout-title  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-titled.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-titled.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-title {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-title {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-title {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-titled .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-titled .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-titled) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-title {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-title {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-title {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-title {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-title {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
<meta property="og:title" content="Creating Small(-ish) LLMs">
<meta property="og:site-name" content="Creating Small(-ish) LLMs">
<meta name="twitter:title" content="Creating Small(-ish) LLMs">
<meta name="twitter:image" content="https://saforem2.github.io/LLM-tutorial/assets/thumbnail.png">
<meta name="twitter:creator" content="@saforem2">
<meta name="twitter:site" content="@saforem2">
<meta name="twitter:image-height" content="2572">
<meta name="twitter:image-width" content="4112">
<meta name="twitter:card" content="summary_large_image">
<meta name="citation_title" content="Creating Small(-ish) LLMs">
<meta name="citation_author" content="Sam Foreman">
<meta name="citation_publication_date" content="2023-11-30">
<meta name="citation_cover_date" content="2023-11-30">
<meta name="citation_year" content="2023">
<meta name="citation_online_date" content="2023-11-30">
<meta name="citation_fulltext_html_url" content="https://saforem2.github.io/LLM-tutorial">
<meta name="citation_language" content="en">
<meta name="citation_reference" content="citation_title=Progress on (g-2)_\mu from lattice QCD;,citation_author=Hartmut Wittig;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2306.04165;">
<meta name="citation_reference" content="citation_title=Hybrid Monte Carlo;,citation_author=S. Duane;,citation_author=A. D. Kennedy;,citation_author=B. J. Pendleton;,citation_author=D. Roweth;,citation_publication_date=1987;,citation_cover_date=1987;,citation_year=1987;,citation_doi=10.1016/0370-2693(87)91197-X;,citation_volume=195;,citation_journal_title=Phys. Lett. B;">
<meta name="citation_reference" content="citation_title=Snowmass 2021 Computational Frontier CompF03 Topical Group Report: Machine Learning;,citation_author=Phiala Shanahan;,citation_author=others;,citation_publication_date=2022-09;,citation_cover_date=2022-09;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2209.07559;">
<meta name="citation_reference" content="citation_title=Applications of Machine Learning to Lattice Quantum Field Theory;,citation_author=Denis Boyda;,citation_author=others;,citation_publication_date=2022-02;,citation_cover_date=2022-02;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2202.05838;,citation_conference_title=Snowmass 2021;">
<meta name="citation_reference" content="citation_title=LeapfrogLayers: A Trainable Framework for Effective Topological Sampling;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2022-05;,citation_cover_date=2022-05;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01582;,citation_doi=10.22323/1.396.0508;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=HMC with Normalizing Flows;,citation_author=Sam Foreman;,citation_author=Taku Izubuchi;,citation_author=Luchang Jin;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_author=Akio Tomiya;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://arxiv.org/abs/2112.01586;,citation_doi=10.22323/1.396.0073;,citation_volume=LATTICE2021;,citation_journal_title=PoS;">
<meta name="citation_reference" content="citation_title=Deep Learning Hamiltonian Monte Carlo;,citation_author=Sam Foreman;,citation_author=Xiao-Yong Jin;,citation_author=James C. Osborn;,citation_publication_date=2021-05;,citation_cover_date=2021-05;,citation_year=2021;,citation_fulltext_html_url=https://arxiv.org/abs/2105.03418;,citation_conference_title=9th International Conference on Learning Representations;">
<meta name="citation_reference" content="citation_title=Mastering language models;,citation_author=Samuel Montgomery;,citation_publication_date=2023-10;,citation_cover_date=2023-10;,citation_year=2023;,citation_fulltext_html_url=https://towardsdatascience.com/mastering-language-models-32e1d891511a;,citation_journal_title=Medium;,citation_publisher=Towards Data Science;">
<meta name="citation_reference" content="citation_title=Harnessing the power of LLMs in practice: A survey on ChatGPT and beyond;,citation_author=Jingfeng Yang;,citation_author=Hongye Jin;,citation_author=Ruixiang Tang;,citation_author=Xiaotian Han;,citation_author=Qizhang Feng;,citation_author=Haoming Jiang;,citation_author=Bing Yin;,citation_author=Xia Hu;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2304.13712;">
<meta name="citation_reference" content="citation_title=Training tips for the transformer model;,citation_author=Martin Popel;,citation_author=OndÅ™ej Bojar;,citation_publication_date=2018-04;,citation_cover_date=2018-04;,citation_year=2018;,citation_fulltext_html_url=https://doi.org/10.2478%2Fpralin-2018-0002;,citation_issue=1;,citation_doi=10.2478/pralin-2018-0002;,citation_volume=110;,citation_journal_title=The Prague Bulletin of Mathematical Linguistics;,citation_publisher=Charles University in Prague, Karolinum Press;">
<meta name="citation_reference" content="citation_title=Attention is all you need;,citation_author=Ashish Vaswani;,citation_author=Noam Shazeer;,citation_author=Niki Parmar;,citation_author=Jakob Uszkoreit;,citation_author=Llion Jones;,citation_author=Aidan N. Gomez;,citation_author=Lukasz Kaiser;,citation_author=Illia Polosukhin;,citation_publication_date=2017;,citation_cover_date=2017;,citation_year=2017;,citation_fulltext_html_url=https://arxiv.org/abs/1706.03762;">
<meta name="citation_reference" content="citation_title=Tree of thoughts: Deliberate problem solving with large language models;,citation_author=Shunyu Yao;,citation_author=Dian Yu;,citation_author=Jeffrey Zhao;,citation_author=Izhak Shafran;,citation_author=Thomas L. Griffiths;,citation_author=Yuan Cao;,citation_author=Karthik Narasimhan;,citation_publication_date=2023;,citation_cover_date=2023;,citation_year=2023;,citation_fulltext_html_url=https://arxiv.org/abs/2305.10601;">
<meta name="citation_reference" content="citation_title=GenSLMs: Genome-scale language models reveal SARS-CoV-2 evolutionary dynamics;,citation_abstract=We seek to transform how new and emergent variants of pandemiccausing viruses, specifically SARS-CoV-2, are identified and classified. By adapting large language models (LLMs) for genomic data, we build genome-scale language models (GenSLMs) which can learn the evolutionary landscape of SARS-CoV-2 genomes. By pretraining on over 110 million prokaryotic gene sequences and finetuning a SARS-CoV-2-specific model on 1.5 million genomes, we show that GenSLMs can accurately and rapidly identify variants of concern. Thus, to our knowledge, GenSLMs represents one of the first whole genome scale foundation models which can generalize to other prediction tasks. We demonstrate scaling of GenSLMs on GPU-based supercomputers and AI-hardware accelerators utilizing 1.63 Zettaflops in training runs with a sustained performance of 121 PFLOPS in mixed precision and peak of 850 PFLOPS. We present initial scientific insights from examining GenSLMs in tracking evolutionary dynamics of SARS-CoV-2, paving the path to realizing this on large biological data.Competing Interest StatementThe authors have declared no competing interest.;,citation_author=Maxim Zvyagin;,citation_author=Alexander Brace;,citation_author=Kyle Hippe;,citation_author=Yuntian Deng;,citation_author=Bin Zhang;,citation_author=Cindy Orozco Bohorquez;,citation_author=Austin Clyde;,citation_author=Bharat Kale;,citation_author=Danilo Perez-Rivera;,citation_author=Heng Ma;,citation_author=Carla M. Mann;,citation_author=Michael Irvin;,citation_author=J. Gregory Pauloski;,citation_author=Logan Ward;,citation_author=Valerie Hayot-Sasson;,citation_author=Murali Emani;,citation_author=Sam Foreman;,citation_author=Zhen Xie;,citation_author=Diangen Lin;,citation_author=Maulik Shukla;,citation_author=Weili Nie;,citation_author=Josh Romero;,citation_author=Christian Dallago;,citation_author=Arash Vahdat;,citation_author=Chaowei Xiao;,citation_author=Thomas Gibbs;,citation_author=Ian Foster;,citation_author=James J. Davis;,citation_author=Michael E. Papka;,citation_author=Thomas Brettin;,citation_author=Rick Stevens;,citation_author=Anima Anandkumar;,citation_author=Venkatram Vishwanath;,citation_author=Arvind Ramanathan;,citation_publication_date=2022;,citation_cover_date=2022;,citation_year=2022;,citation_fulltext_html_url=https://www.biorxiv.org/content/early/2022/11/23/2022.10.10.511571;,citation_doi=10.1101/2022.10.10.511571;,citation_journal_title=bioRxiv;,citation_publisher=Cold Spring Harbor Laboratory;">
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="quarto-title-block center">
  <h1 class="title">Creating Small(-ish) LLMs</h1>

<div class="quarto-title-authors">
<div class="quarto-title-author">
<div class="quarto-title-author-name">
<a href="https://samforeman.me">Sam Foreman </a><a href="https://orcid.org/0000-0002-9981-0876"><span class="orcid-green"><i class="ai  ai-orcid"></i></span></a> 
</div>
<div class="quarto-title-author-email">
<a href="mailto:foremans@anl.gov">foremans@anl.gov</a>
</div>
        <p class="quarto-title-affiliation">
            <a href="https://alcf.anl.gov/about/people/sam-foreman">
            Argonne National Laboratory
            </a>
          </p>
    </div>
</div>

  <p class="date">2023-11-30</p>
</section>
<section id="creating-small-ish-llmsslides-gh" class="title-slide slide level1 center">
<h1>Creating Small(-ish) LLMs<sup>1</sup></h1>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-bottom">
<div class="col1 quarto-layout-cell" style="flex-basis: 55.6%;justify-content: center;">
<p><span class="dim-text"><strong>LLM Tutorial / Workshop</strong></span><br>
<span class="dim-text">Argonne National Laboratory</span><br>
<span class="dim-text">Building 240, Room 1501</span><br>
<br></p>
<p><span style="font-weight: 600;"><a href="https://samforeman.me"><i class="bi-person-badge " style="" role="img" aria-hidden="true"></i>Sam Foreman</a></span><br>
<span class="dim-text" style="font-size: 0.8em;">2023-11-30</span></p>
<ul>
<li><a href="https://github.com/brettin/llm_tutorial"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>brettin/llm_tutorial</code></a></li>
<li><a href="https://github.com/saforem2"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>saforem2/</code></a>
<ul>
<li><a href="https://saforem2.github.io/nanoGPT"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>nanoGPT</code></a> (GitHub)</li>
<li><a href="https://saforem2.github.io/LLM-tutorial"><i class="bi-easel " style="" role="img" aria-hidden="true"></i> <code>LLM-tutorial</code></a> (slides)</li>
</ul></li>
</ul>
</div>
<div id="fig-llms" class="quarto-figure quarto-figure-center" style="flex-basis: 44.4%;justify-content: center;">
<figure>
<p><img data-src="https://github.com/Hannibal046/Awesome-LLM/raw/main/resources/image8.gif"></p>
<figcaption>Figure&nbsp;1: <span class="dim-text">Large Language Models have (LLM)s have taken the <del>NLP community</del> <strong>world</strong> by storm<sup>2</sup></span></figcaption>
</figure>
</div>
</div>
</div>
<aside><ol class="aside-footnotes"><li id="fn1"><p><a href="https://github.com/saforem2/LLM-tutorial"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>saforem2/LLM-tutorial</code></a></p></li><li id="fn2"><p><a href="https://github.com/Hannibal046/Awesome-LLM"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>Hannibal046/Awesome-LLM</code></a></p></li></ol></aside></section>


<section id="emergent-abilities" class="title-slide slide level1 center" data-background-color="#FBFBFD">
<h1>Emergent Abilities</h1>
<div width="66%" style="text-align: center;">
<p><img src="https://github.com/saforem2/llm-lunch-talk/blob/main/docs/assets/emergent-abilities.gif?raw=true" height="75%"></p>
<p><a href="https://arxiv.org/abs/2206.07682">Emergent abilities of Large Language Models</a> <span class="citation" data-cites="yao2023tree">Yao et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></p>
</div>
</section>

<section id="training-llms" class="title-slide slide level1 center">
<h1>Training LLMs</h1>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-center">
<div id="fig-evolution" class="quarto-figure quarto-figure-center" style="flex-basis: 55.6%;justify-content: center;">
<figure>
<p><img data-src="https://github.com/Mooler0410/LLMsPracticalGuide/raw/main/imgs/survey-gif-test.gif"></p>
<figcaption>Figure&nbsp;2: Visualization from <span class="citation" data-cites="yang2023harnessing">Yang et al. (<a href="#/references" role="doc-biblioref" onclick="">2023</a>)</span></figcaption>
</figure>
</div>
<div class="quarto-layout-cell" style="flex-basis: 44.4%;justify-content: center;">
<p><img data-src="https://github.com/saforem2/llm-lunch-talk/blob/main/docs/assets/it_hungers.jpeg?raw=true"></p>
</div>
</div>
</div>
</section>

<section id="life-cycle-of-the-llm" class="title-slide slide level1 center" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Life-Cycle of the LLM</h1>
<div class="quarto-layout-panel">
<div class="quarto-layout-row quarto-layout-valign-center">
<div id="column-one" class="quarto-layout-cell" style="flex-basis: 45.0%;justify-content: center;">
<ol type="1">
<li><p>Data collection + preprocessing</p></li>
<li><p><strong>Pre-training</strong></p>
<ul>
<li>Architecture decisions:<br>
<code>{model_size, hyperparameters,</code><br>
<code>parallelism, lr_schedule, ...}</code></li>
</ul></li>
<li><p>Supervised Fine-Tuning</p>
<ul>
<li>Instruction Tuning</li>
<li>Alignment</li>
</ul></li>
<li><p>Deploy (+ monitor, re-evaluate, etc.)</p></li>
</ol>
</div>
<div id="fig-pretrain-two" class="quarto-figure quarto-figure-center" style="flex-basis: 55.0%;justify-content: center;">
<figure>
<p><img data-src="https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif"></p>
<figcaption>Figure&nbsp;3: <strong>Pre-training</strong>: Virtually all of the compute used during pretraining phase<sup>1</sup>.</figcaption>
</figure>
</div>
</div>
</div>
<aside><ol class="aside-footnotes"><li id="fn3"><p>Figure from <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p></li></ol></aside></section>


<section id="forward-pass" class="title-slide slide level1 center">
<h1>Forward Pass</h1>
<div id="fig-forward-pass" class="quarto-figure quarto-figure-center">
<figure>
<video data-autoplay="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov">
</video>
<figcaption>Figure&nbsp;4: Language Model trained for causal language modeling. Video from: <a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial">ðŸ¤— Generation with LLMs</a></figcaption>
</figure>
</div>
</section>

<section id="generating-text" class="title-slide slide level1 center">
<h1>Generating Text</h1>
<div id="fig-generating-text" class="quarto-figure quarto-figure-center">
<figure>
<video data-autoplay="" src="https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_2_1080p.mov">
</video>
<figcaption>Figure&nbsp;5: Language Model trained for causal language modeling. Video from: <a href="https://huggingface.co/docs/transformers/main/en/llm_tutorial">ðŸ¤— Generation with LLMs</a></figcaption>
</figure>
</div>
</section>

<section id="life-cycle-of-the-llm-pre-training" class="title-slide slide level1 center" data-auto-animate="true">
<h1 data-id="quarto-animate-title">Life-Cycle of the LLM: Pre-training</h1>

<img data-src="https://jalammar.github.io/images/gpt3/03-gpt3-training-step-back-prop.gif" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;6: <strong>Pre-training</strong>: Virtually all of the compute used during pretraining phase</p></section>

<section id="life-cycle-of-the-llm-fine-tuning" class="title-slide slide level1 center" data-auto-animate="true" style="font-size: 0.8em;">
<h1 data-id="quarto-animate-title">Life-Cycle of the LLM: Fine-Tuning</h1>
<div id="fig-pretrain-two" class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="https://jalammar.github.io/images/gpt3/10-gpt3-fine-tuning.gif"></p>
<figcaption>Figure&nbsp;7: <strong>Fine-tuning</strong><sup>1</sup>: Fine-tuning actually updates the modelâ€™s weights to make the model better at a certain task.</figcaption>
</figure>
</div>
<aside><ol class="aside-footnotes"><li id="fn4"><p>Figure from <a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></p></li></ol></aside></section>


<section id="assistant-models" class="title-slide slide level1 centeredslide center" data-background-color="#181D29">
<h1>Assistant Models</h1>

<img data-src="./assets/jailbreak.jpeg" class="r-stretch"></section>

<section id="iconify-line-md-github-loop-saforem2nanogpt" class="title-slide slide level1 center">
<h1><a href="https://github.com/saforem2/nanoGPT"><iconify-icon inline="" icon="line-md:github-loop"></iconify-icon><code>saforem2/nanoGPT</code></a></h1>
<!-- - [ `saforem2/nanoGPT`](https://github.com/saforem2/nanoGPT) -->
<ul>
<li>Fork of Andrej Karpathyâ€™s <code>nanoGPT</code></li>
</ul>

<img data-src="https://github.com/saforem2/nanoGPT/raw/master/assets/nanogpt.jpg" class="r-stretch quarto-figure-center"><p class="caption">Figure&nbsp;8: The simplest, fastest repository for training / finetuning GPT based models.</p></section>

<section id="install" class="title-slide slide level1 center">
<h1>Install</h1>
<div class="sourceCode" id="cb1"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="fu">git</span> clone https://github.com/saforem2/nanoGPT</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="bu">cd</span> nanoGPT</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">mkdir</span> <span class="at">-p</span> venv</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> venv venv <span class="at">--system-site-packages</span></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="bu">source</span> venv/bin/activate</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-m</span> pip install <span class="at">-e</span> .</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> <span class="at">-c</span> <span class="st">'import ngpt; print(ngpt.__file__)'</span></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="co"># ./nanoGPT/src/ngpt/__init__.py</span></span></code></pre></div>
</section>

<section id="dependencies" class="title-slide slide level1 center">
<h1>Dependencies</h1>
<ul>
<li><a href="https://github.com/huggingface/transformers"><code>transformers</code></a> for <iconify-icon inline="" icon="noto:hugging-face"></iconify-icon> transformers (to load <code>GPT-2</code> checkpoints)</li>
<li><a href="https://github.com/huggingface/datasets"><code>datasets</code></a> for <iconify-icon inline="" icon="noto:hugging-face"></iconify-icon> datasets (if you want to use OpenWebText)</li>
<li><a href="https://github.com/openai/tiktoken"><code>tiktoken</code></a> for OpenAIâ€™s fast BPE code</li>
<li><a href="https://wandb.ai"><code>wandb</code></a> for optional logging</li>
<li><a href="https://github.com/tqdm/tqdm"><code>tqdm</code></a> for progress bars</li>
</ul>
</section>

<section id="quick-start" class="title-slide slide level1 center">
<h1>Quick Start</h1>
<ul>
<li><p>We start with training a character-level GPT on the works of Shakespeare.</p>
<ol type="1">
<li>Downloading the data (~ 1MB) file</li>
<li>Convert raw text to one large stream of integers</li>
</ol>
<div class="sourceCode" id="cb2"><pre class="sourceCode bash"><code class="sourceCode bash"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="ex">python3</span> data/shakespeare_char/prepare.py</span></code></pre></div>
<p>This will create <code>data/shakespeare_char/{train.bin, val.bin}</code>.</p></li>
</ul>
</section>

<section id="iconify-fa-brands-github-model.py" class="title-slide slide level1 center" height="100%">
<h1><a href="https://github.com/saforem2/nanoGPT/blob/master/src/ngpt/model.py"><iconify-icon inline="" icon="fa-brands:github"></iconify-icon> <code>model.py</code></a></h1>
<!-- ::: {style="font-size: 0.75em;"} -->
<div class="panel-tabset" style="font-size: 0.75em; width: 100%!important; height: 100%!important;">
<ul id="tabset-1" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-1-1"><code>GPT</code></a></li><li><a href="#tabset-1-2"><code>LayerNorm</code></a></li><li><a href="#tabset-1-3"><code>CausalSelfAttention</code></a></li><li><a href="#tabset-1-4"><code>MLP</code></a></li></ul>
<div class="tab-content" style="font-size: 0.75em; width: 100%!important; height: 100%!important;">
<div id="tabset-1-1">
<div class="sourceCode" id="cb3" data-code-line-numbers="true" data-startfrom="131"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 130;"><span id="cb3-131"><a href="#cb3-131"></a><span class="kw">class</span> GPT(nn.Module):</span>
<span id="cb3-132"><a href="#cb3-132"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb3-133"><a href="#cb3-133"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb3-134"><a href="#cb3-134"></a>        <span class="cf">assert</span> config.vocab_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb3-135"><a href="#cb3-135"></a>        <span class="cf">assert</span> config.block_size <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span></span>
<span id="cb3-136"><a href="#cb3-136"></a>        <span class="va">self</span>.config <span class="op">=</span> config</span>
<span id="cb3-137"><a href="#cb3-137"></a></span>
<span id="cb3-138"><a href="#cb3-138"></a>        <span class="va">self</span>.transformer <span class="op">=</span> nn.ModuleDict(<span class="bu">dict</span>(</span>
<span id="cb3-139"><a href="#cb3-139"></a>            wte <span class="op">=</span> nn.Embedding(config.vocab_size, config.n_embd),</span>
<span id="cb3-140"><a href="#cb3-140"></a>            wpe <span class="op">=</span> nn.Embedding(config.block_size, config.n_embd),</span>
<span id="cb3-141"><a href="#cb3-141"></a>            drop <span class="op">=</span> nn.Dropout(config.dropout),</span>
<span id="cb3-142"><a href="#cb3-142"></a>            h <span class="op">=</span> nn.ModuleList([Block(config) <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(config.n_layer)]),</span>
<span id="cb3-143"><a href="#cb3-143"></a>            ln_f <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias),</span>
<span id="cb3-144"><a href="#cb3-144"></a>        ))</span>
<span id="cb3-145"><a href="#cb3-145"></a>        <span class="va">self</span>.lm_head <span class="op">=</span> nn.Linear(config.n_embd, config.vocab_size, bias<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb3-146"><a href="#cb3-146"></a>        <span class="co"># with weight tying when using torch.compile() some warnings get generated:</span></span>
<span id="cb3-147"><a href="#cb3-147"></a>        <span class="co"># "UserWarning: functional_call was passed multiple values for tied weights.</span></span>
<span id="cb3-148"><a href="#cb3-148"></a>        <span class="co"># This behavior is deprecated and will be an error in future versions"</span></span>
<span id="cb3-149"><a href="#cb3-149"></a>        <span class="co"># not 100% sure what this is, so far seems to be harmless. </span><span class="al">TODO</span><span class="co"> investigate</span></span>
<span id="cb3-150"><a href="#cb3-150"></a>        <span class="va">self</span>.transformer.wte.weight <span class="op">=</span> <span class="va">self</span>.lm_head.weight <span class="co"># https://paperswithcode.com/method/weight-tying</span></span>
<span id="cb3-151"><a href="#cb3-151"></a></span>
<span id="cb3-152"><a href="#cb3-152"></a>        <span class="co"># init all weights</span></span>
<span id="cb3-153"><a href="#cb3-153"></a>        <span class="va">self</span>.<span class="bu">apply</span>(<span class="va">self</span>._init_weights)</span>
<span id="cb3-154"><a href="#cb3-154"></a>        <span class="co"># apply special scaled init to the residual projections, per GPT-2 paper</span></span>
<span id="cb3-155"><a href="#cb3-155"></a>        <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters():</span>
<span id="cb3-156"><a href="#cb3-156"></a>            <span class="cf">if</span> pn.endswith(<span class="st">'c_proj.weight'</span>):</span>
<span id="cb3-157"><a href="#cb3-157"></a>                torch.nn.init.normal_(p, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span><span class="op">/</span>math.sqrt(<span class="dv">2</span> <span class="op">*</span> config.n_layer))</span>
<span id="cb3-158"><a href="#cb3-158"></a></span>
<span id="cb3-159"><a href="#cb3-159"></a>        <span class="co"># report number of parameters</span></span>
<span id="cb3-160"><a href="#cb3-160"></a>        log.info(<span class="st">"number of parameters: </span><span class="sc">%.2f</span><span class="st">M"</span> <span class="op">%</span> (<span class="va">self</span>.get_num_params()<span class="op">/</span><span class="fl">1e6</span>,))</span>
<span id="cb3-161"><a href="#cb3-161"></a></span>
<span id="cb3-162"><a href="#cb3-162"></a>    <span class="kw">def</span> get_num_params(<span class="va">self</span>, non_embedding<span class="op">=</span><span class="va">True</span>):</span>
<span id="cb3-163"><a href="#cb3-163"></a>        <span class="co">"""</span></span>
<span id="cb3-164"><a href="#cb3-164"></a><span class="co">        Return the number of parameters in the model.</span></span>
<span id="cb3-165"><a href="#cb3-165"></a><span class="co">        For non-embedding count (default), the position embeddings get subtracted.</span></span>
<span id="cb3-166"><a href="#cb3-166"></a><span class="co">        The token embeddings would too, except due to the parameter sharing these</span></span>
<span id="cb3-167"><a href="#cb3-167"></a><span class="co">        params are actually used as weights in the final layer, so we include them.</span></span>
<span id="cb3-168"><a href="#cb3-168"></a><span class="co">        """</span></span>
<span id="cb3-169"><a href="#cb3-169"></a>        n_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> <span class="va">self</span>.parameters())</span>
<span id="cb3-170"><a href="#cb3-170"></a>        <span class="cf">if</span> non_embedding:</span>
<span id="cb3-171"><a href="#cb3-171"></a>            n_params <span class="op">-=</span> <span class="va">self</span>.transformer.wpe.weight.numel()</span>
<span id="cb3-172"><a href="#cb3-172"></a>        <span class="cf">return</span> n_params</span>
<span id="cb3-173"><a href="#cb3-173"></a></span>
<span id="cb3-174"><a href="#cb3-174"></a>    <span class="kw">def</span> _init_weights(<span class="va">self</span>, module):</span>
<span id="cb3-175"><a href="#cb3-175"></a>        <span class="cf">if</span> <span class="bu">isinstance</span>(module, nn.Linear):</span>
<span id="cb3-176"><a href="#cb3-176"></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb3-177"><a href="#cb3-177"></a>            <span class="cf">if</span> module.bias <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-178"><a href="#cb3-178"></a>                torch.nn.init.zeros_(module.bias)</span>
<span id="cb3-179"><a href="#cb3-179"></a>        <span class="cf">elif</span> <span class="bu">isinstance</span>(module, nn.Embedding):</span>
<span id="cb3-180"><a href="#cb3-180"></a>            torch.nn.init.normal_(module.weight, mean<span class="op">=</span><span class="fl">0.0</span>, std<span class="op">=</span><span class="fl">0.02</span>)</span>
<span id="cb3-181"><a href="#cb3-181"></a></span>
<span id="cb3-182"><a href="#cb3-182"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, idx, targets<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-183"><a href="#cb3-183"></a>        device <span class="op">=</span> idx.device</span>
<span id="cb3-184"><a href="#cb3-184"></a>        b, t <span class="op">=</span> idx.size()</span>
<span id="cb3-185"><a href="#cb3-185"></a>        <span class="cf">assert</span> t <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size, <span class="ss">f"Cannot forward sequence of length </span><span class="sc">{</span>t<span class="sc">}</span><span class="ss">, block size is only </span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>config<span class="sc">.</span>block_size<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-186"><a href="#cb3-186"></a>        pos <span class="op">=</span> torch.arange(<span class="dv">0</span>, t, dtype<span class="op">=</span>torch.<span class="bu">long</span>, device<span class="op">=</span>device) <span class="co"># shape (t)</span></span>
<span id="cb3-187"><a href="#cb3-187"></a></span>
<span id="cb3-188"><a href="#cb3-188"></a>        <span class="co"># forward the GPT model itself</span></span>
<span id="cb3-189"><a href="#cb3-189"></a>        tok_emb <span class="op">=</span> <span class="va">self</span>.transformer.wte(idx) <span class="co"># token embeddings of shape (b, t, n_embd)</span></span>
<span id="cb3-190"><a href="#cb3-190"></a>        pos_emb <span class="op">=</span> <span class="va">self</span>.transformer.wpe(pos) <span class="co"># position embeddings of shape (t, n_embd)</span></span>
<span id="cb3-191"><a href="#cb3-191"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.drop(tok_emb <span class="op">+</span> pos_emb)</span>
<span id="cb3-192"><a href="#cb3-192"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:</span>
<span id="cb3-193"><a href="#cb3-193"></a>            x <span class="op">=</span> block(x)</span>
<span id="cb3-194"><a href="#cb3-194"></a>        x <span class="op">=</span> <span class="va">self</span>.transformer.ln_f(x)</span>
<span id="cb3-195"><a href="#cb3-195"></a></span>
<span id="cb3-196"><a href="#cb3-196"></a>        <span class="cf">if</span> targets <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-197"><a href="#cb3-197"></a>            <span class="co"># if we are given some desired targets also calculate the loss</span></span>
<span id="cb3-198"><a href="#cb3-198"></a>            logits <span class="op">=</span> <span class="va">self</span>.lm_head(x)</span>
<span id="cb3-199"><a href="#cb3-199"></a>            loss <span class="op">=</span> F.cross_entropy(logits.view(<span class="op">-</span><span class="dv">1</span>, logits.size(<span class="op">-</span><span class="dv">1</span>)), targets.view(<span class="op">-</span><span class="dv">1</span>), ignore_index<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-200"><a href="#cb3-200"></a>        <span class="cf">else</span>:</span>
<span id="cb3-201"><a href="#cb3-201"></a>            <span class="co"># inference-time mini-optimization: only forward the lm_head on the very last position</span></span>
<span id="cb3-202"><a href="#cb3-202"></a>            logits <span class="op">=</span> <span class="va">self</span>.lm_head(x[:, [<span class="op">-</span><span class="dv">1</span>], :]) <span class="co"># note: using list [-1] to preserve the time dim</span></span>
<span id="cb3-203"><a href="#cb3-203"></a>            loss <span class="op">=</span> <span class="va">None</span></span>
<span id="cb3-204"><a href="#cb3-204"></a></span>
<span id="cb3-205"><a href="#cb3-205"></a>        <span class="cf">return</span> logits, loss</span>
<span id="cb3-206"><a href="#cb3-206"></a></span>
<span id="cb3-207"><a href="#cb3-207"></a>    <span class="kw">def</span> crop_block_size(<span class="va">self</span>, block_size):</span>
<span id="cb3-208"><a href="#cb3-208"></a>        <span class="co"># model surgery to decrease the block size if necessary</span></span>
<span id="cb3-209"><a href="#cb3-209"></a>        <span class="co"># e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)</span></span>
<span id="cb3-210"><a href="#cb3-210"></a>        <span class="co"># but want to use a smaller block size for some smaller, simpler model</span></span>
<span id="cb3-211"><a href="#cb3-211"></a>        <span class="cf">assert</span> block_size <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size</span>
<span id="cb3-212"><a href="#cb3-212"></a>        <span class="va">self</span>.config.block_size <span class="op">=</span> block_size</span>
<span id="cb3-213"><a href="#cb3-213"></a>        <span class="va">self</span>.transformer.wpe.weight <span class="op">=</span> nn.Parameter(<span class="va">self</span>.transformer.wpe.weight[:block_size])</span>
<span id="cb3-214"><a href="#cb3-214"></a>        <span class="cf">for</span> block <span class="kw">in</span> <span class="va">self</span>.transformer.h:</span>
<span id="cb3-215"><a href="#cb3-215"></a>            <span class="cf">if</span> <span class="bu">hasattr</span>(block.attn, <span class="st">'bias'</span>):</span>
<span id="cb3-216"><a href="#cb3-216"></a>                block.attn.bias <span class="op">=</span> block.attn.bias[:,:,:block_size,:block_size]</span>
<span id="cb3-217"><a href="#cb3-217"></a></span>
<span id="cb3-218"><a href="#cb3-218"></a>    <span class="at">@classmethod</span></span>
<span id="cb3-219"><a href="#cb3-219"></a>    <span class="kw">def</span> from_pretrained(cls, model_type, override_args<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-220"><a href="#cb3-220"></a>        <span class="cf">assert</span> model_type <span class="kw">in</span> {<span class="st">'gpt2'</span>, <span class="st">'gpt2-medium'</span>, <span class="st">'gpt2-large'</span>, <span class="st">'gpt2-xl'</span>}</span>
<span id="cb3-221"><a href="#cb3-221"></a>        override_args <span class="op">=</span> override_args <span class="kw">or</span> {} <span class="co"># default to empty dict</span></span>
<span id="cb3-222"><a href="#cb3-222"></a>        <span class="co"># only dropout can be overridden see more notes below</span></span>
<span id="cb3-223"><a href="#cb3-223"></a>        <span class="cf">assert</span> <span class="bu">all</span>(k <span class="op">==</span> <span class="st">'dropout'</span> <span class="cf">for</span> k <span class="kw">in</span> override_args)</span>
<span id="cb3-224"><a href="#cb3-224"></a>        <span class="im">from</span> transformers <span class="im">import</span> GPT2LMHeadModel</span>
<span id="cb3-225"><a href="#cb3-225"></a>        log.info(<span class="st">"loading weights from pretrained gpt: </span><span class="sc">%s</span><span class="st">"</span> <span class="op">%</span> model_type)</span>
<span id="cb3-226"><a href="#cb3-226"></a></span>
<span id="cb3-227"><a href="#cb3-227"></a>        <span class="co"># n_layer, n_head and n_embd are determined from model_type</span></span>
<span id="cb3-228"><a href="#cb3-228"></a>        config_args <span class="op">=</span> {</span>
<span id="cb3-229"><a href="#cb3-229"></a>            <span class="st">'gpt2'</span>:         <span class="bu">dict</span>(n_layer<span class="op">=</span><span class="dv">12</span>, n_head<span class="op">=</span><span class="dv">12</span>, n_embd<span class="op">=</span><span class="dv">768</span>),  <span class="co"># 124M params</span></span>
<span id="cb3-230"><a href="#cb3-230"></a>            <span class="st">'gpt2-medium'</span>:  <span class="bu">dict</span>(n_layer<span class="op">=</span><span class="dv">24</span>, n_head<span class="op">=</span><span class="dv">16</span>, n_embd<span class="op">=</span><span class="dv">1024</span>), <span class="co"># 350M params</span></span>
<span id="cb3-231"><a href="#cb3-231"></a>            <span class="st">'gpt2-large'</span>:   <span class="bu">dict</span>(n_layer<span class="op">=</span><span class="dv">36</span>, n_head<span class="op">=</span><span class="dv">20</span>, n_embd<span class="op">=</span><span class="dv">1280</span>), <span class="co"># 774M params</span></span>
<span id="cb3-232"><a href="#cb3-232"></a>            <span class="st">'gpt2-xl'</span>:      <span class="bu">dict</span>(n_layer<span class="op">=</span><span class="dv">48</span>, n_head<span class="op">=</span><span class="dv">25</span>, n_embd<span class="op">=</span><span class="dv">1600</span>), <span class="co"># 1558M params</span></span>
<span id="cb3-233"><a href="#cb3-233"></a>        }[model_type]</span>
<span id="cb3-234"><a href="#cb3-234"></a>        log.info(<span class="st">"forcing vocab_size=50257, block_size=1024, bias=True"</span>)</span>
<span id="cb3-235"><a href="#cb3-235"></a>        config_args[<span class="st">'vocab_size'</span>] <span class="op">=</span> <span class="dv">50257</span> <span class="co"># always 50257 for GPT model checkpoints</span></span>
<span id="cb3-236"><a href="#cb3-236"></a>        config_args[<span class="st">'block_size'</span>] <span class="op">=</span> <span class="dv">1024</span> <span class="co"># always 1024 for GPT model checkpoints</span></span>
<span id="cb3-237"><a href="#cb3-237"></a>        config_args[<span class="st">'bias'</span>] <span class="op">=</span> <span class="va">True</span> <span class="co"># always True for GPT model checkpoints</span></span>
<span id="cb3-238"><a href="#cb3-238"></a>        <span class="co"># we can override the dropout rate, if desired</span></span>
<span id="cb3-239"><a href="#cb3-239"></a>        <span class="cf">if</span> <span class="st">'dropout'</span> <span class="kw">in</span> override_args:</span>
<span id="cb3-240"><a href="#cb3-240"></a>            log.info(<span class="ss">f"overriding dropout rate to </span><span class="sc">{</span>override_args[<span class="st">'dropout'</span>]<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-241"><a href="#cb3-241"></a>            config_args[<span class="st">'dropout'</span>] <span class="op">=</span> override_args[<span class="st">'dropout'</span>]</span>
<span id="cb3-242"><a href="#cb3-242"></a>        <span class="co"># create a from-scratch initialized minGPT model</span></span>
<span id="cb3-243"><a href="#cb3-243"></a>        config <span class="op">=</span> ModelConfig(<span class="op">**</span>config_args)</span>
<span id="cb3-244"><a href="#cb3-244"></a>        model <span class="op">=</span> GPT(config)</span>
<span id="cb3-245"><a href="#cb3-245"></a>        sd <span class="op">=</span> model.state_dict()</span>
<span id="cb3-246"><a href="#cb3-246"></a>        sd_keys <span class="op">=</span> sd.keys()</span>
<span id="cb3-247"><a href="#cb3-247"></a>        sd_keys <span class="op">=</span> [k <span class="cf">for</span> k <span class="kw">in</span> sd_keys <span class="cf">if</span> <span class="kw">not</span> k.endswith(<span class="st">'.attn.bias'</span>)] <span class="co"># discard this mask / buffer, not a param</span></span>
<span id="cb3-248"><a href="#cb3-248"></a></span>
<span id="cb3-249"><a href="#cb3-249"></a>        <span class="co"># init a huggingface/transformers model</span></span>
<span id="cb3-250"><a href="#cb3-250"></a>        model_hf <span class="op">=</span> GPT2LMHeadModel.from_pretrained(model_type)</span>
<span id="cb3-251"><a href="#cb3-251"></a>        sd_hf <span class="op">=</span> model_hf.state_dict()</span>
<span id="cb3-252"><a href="#cb3-252"></a></span>
<span id="cb3-253"><a href="#cb3-253"></a>        <span class="co"># copy while ensuring all of the parameters are aligned and match in names and shapes</span></span>
<span id="cb3-254"><a href="#cb3-254"></a>        sd_keys_hf <span class="op">=</span> sd_hf.keys()</span>
<span id="cb3-255"><a href="#cb3-255"></a>        sd_keys_hf <span class="op">=</span> [k <span class="cf">for</span> k <span class="kw">in</span> sd_keys_hf <span class="cf">if</span> <span class="kw">not</span> k.endswith(<span class="st">'.attn.masked_bias'</span>)] <span class="co"># ignore these, just a buffer</span></span>
<span id="cb3-256"><a href="#cb3-256"></a>        sd_keys_hf <span class="op">=</span> [k <span class="cf">for</span> k <span class="kw">in</span> sd_keys_hf <span class="cf">if</span> <span class="kw">not</span> k.endswith(<span class="st">'.attn.bias'</span>)] <span class="co"># same, just the mask (buffer)</span></span>
<span id="cb3-257"><a href="#cb3-257"></a>        transposed <span class="op">=</span> [<span class="st">'attn.c_attn.weight'</span>, <span class="st">'attn.c_proj.weight'</span>, <span class="st">'mlp.c_fc.weight'</span>, <span class="st">'mlp.c_proj.weight'</span>]</span>
<span id="cb3-258"><a href="#cb3-258"></a>        <span class="co"># basically the openai checkpoints use a "Conv1D" module, but we only want to use a vanilla Linear</span></span>
<span id="cb3-259"><a href="#cb3-259"></a>        <span class="co"># this means that we have to transpose these weights when we import them</span></span>
<span id="cb3-260"><a href="#cb3-260"></a>        <span class="cf">assert</span> <span class="bu">len</span>(sd_keys_hf) <span class="op">==</span> <span class="bu">len</span>(sd_keys), <span class="ss">f"mismatched keys: </span><span class="sc">{</span><span class="bu">len</span>(sd_keys_hf)<span class="sc">}</span><span class="ss"> != </span><span class="sc">{</span><span class="bu">len</span>(sd_keys)<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb3-261"><a href="#cb3-261"></a>        <span class="cf">for</span> k <span class="kw">in</span> sd_keys_hf:</span>
<span id="cb3-262"><a href="#cb3-262"></a>            <span class="cf">if</span> <span class="bu">any</span>(k.endswith(w) <span class="cf">for</span> w <span class="kw">in</span> transposed):</span>
<span id="cb3-263"><a href="#cb3-263"></a>                <span class="co"># special treatment for the Conv1D weights we need to transpose</span></span>
<span id="cb3-264"><a href="#cb3-264"></a>                <span class="cf">assert</span> sd_hf[k].shape[::<span class="op">-</span><span class="dv">1</span>] <span class="op">==</span> sd[k].shape</span>
<span id="cb3-265"><a href="#cb3-265"></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-266"><a href="#cb3-266"></a>                    sd[k].copy_(sd_hf[k].t())</span>
<span id="cb3-267"><a href="#cb3-267"></a>            <span class="cf">else</span>:</span>
<span id="cb3-268"><a href="#cb3-268"></a>                <span class="co"># vanilla copy over the other parameters</span></span>
<span id="cb3-269"><a href="#cb3-269"></a>                <span class="cf">assert</span> sd_hf[k].shape <span class="op">==</span> sd[k].shape</span>
<span id="cb3-270"><a href="#cb3-270"></a>                <span class="cf">with</span> torch.no_grad():</span>
<span id="cb3-271"><a href="#cb3-271"></a>                    sd[k].copy_(sd_hf[k])</span>
<span id="cb3-272"><a href="#cb3-272"></a></span>
<span id="cb3-273"><a href="#cb3-273"></a>        <span class="cf">return</span> model</span>
<span id="cb3-274"><a href="#cb3-274"></a></span>
<span id="cb3-275"><a href="#cb3-275"></a>    <span class="kw">def</span> configure_optimizers(<span class="va">self</span>, weight_decay, learning_rate, betas, device_type):</span>
<span id="cb3-276"><a href="#cb3-276"></a>        <span class="co"># start with all of the candidate parameters</span></span>
<span id="cb3-277"><a href="#cb3-277"></a>        param_dict <span class="op">=</span> {pn: p <span class="cf">for</span> pn, p <span class="kw">in</span> <span class="va">self</span>.named_parameters()}</span>
<span id="cb3-278"><a href="#cb3-278"></a>        <span class="co"># filter out those that do not require grad</span></span>
<span id="cb3-279"><a href="#cb3-279"></a>        param_dict <span class="op">=</span> {pn: p <span class="cf">for</span> pn, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.requires_grad}</span>
<span id="cb3-280"><a href="#cb3-280"></a>        <span class="co"># create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.</span></span>
<span id="cb3-281"><a href="#cb3-281"></a>        <span class="co"># i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.</span></span>
<span id="cb3-282"><a href="#cb3-282"></a>        decay_params <span class="op">=</span> [p <span class="cf">for</span> n, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.dim() <span class="op">&gt;=</span> <span class="dv">2</span>]</span>
<span id="cb3-283"><a href="#cb3-283"></a>        nodecay_params <span class="op">=</span> [p <span class="cf">for</span> n, p <span class="kw">in</span> param_dict.items() <span class="cf">if</span> p.dim() <span class="op">&lt;</span> <span class="dv">2</span>]</span>
<span id="cb3-284"><a href="#cb3-284"></a>        optim_groups <span class="op">=</span> [</span>
<span id="cb3-285"><a href="#cb3-285"></a>            {<span class="st">'params'</span>: decay_params, <span class="st">'weight_decay'</span>: weight_decay},</span>
<span id="cb3-286"><a href="#cb3-286"></a>            {<span class="st">'params'</span>: nodecay_params, <span class="st">'weight_decay'</span>: <span class="fl">0.0</span>}</span>
<span id="cb3-287"><a href="#cb3-287"></a>        ]</span>
<span id="cb3-288"><a href="#cb3-288"></a>        num_decay_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> decay_params)</span>
<span id="cb3-289"><a href="#cb3-289"></a>        num_nodecay_params <span class="op">=</span> <span class="bu">sum</span>(p.numel() <span class="cf">for</span> p <span class="kw">in</span> nodecay_params)</span>
<span id="cb3-290"><a href="#cb3-290"></a>        log.info(<span class="ss">f"num decayed parameter tensors: </span><span class="sc">{</span><span class="bu">len</span>(decay_params)<span class="sc">}</span><span class="ss">, with </span><span class="sc">{</span>num_decay_params<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb3-291"><a href="#cb3-291"></a>        log.info(<span class="ss">f"num non-decayed parameter tensors: </span><span class="sc">{</span><span class="bu">len</span>(nodecay_params)<span class="sc">}</span><span class="ss">, with </span><span class="sc">{</span>num_nodecay_params<span class="sc">:,}</span><span class="ss"> parameters"</span>)</span>
<span id="cb3-292"><a href="#cb3-292"></a>        <span class="co"># Create AdamW optimizer and use the fused version if it is available</span></span>
<span id="cb3-293"><a href="#cb3-293"></a>        fused_available <span class="op">=</span> <span class="st">'fused'</span> <span class="kw">in</span> inspect.signature(torch.optim.AdamW).parameters</span>
<span id="cb3-294"><a href="#cb3-294"></a>        use_fused <span class="op">=</span> fused_available <span class="kw">and</span> device_type <span class="op">==</span> <span class="st">'cuda'</span></span>
<span id="cb3-295"><a href="#cb3-295"></a>        extra_args <span class="op">=</span> <span class="bu">dict</span>(fused<span class="op">=</span><span class="va">True</span>) <span class="cf">if</span> use_fused <span class="cf">else</span> <span class="bu">dict</span>()</span>
<span id="cb3-296"><a href="#cb3-296"></a>        optimizer <span class="op">=</span> torch.optim.AdamW(optim_groups, lr<span class="op">=</span>learning_rate, betas<span class="op">=</span>betas, <span class="op">**</span>extra_args)</span>
<span id="cb3-297"><a href="#cb3-297"></a>        log.info(<span class="ss">f"using fused AdamW: </span><span class="sc">{</span>use_fused<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb3-298"><a href="#cb3-298"></a></span>
<span id="cb3-299"><a href="#cb3-299"></a>        <span class="cf">return</span> optimizer</span>
<span id="cb3-300"><a href="#cb3-300"></a></span>
<span id="cb3-301"><a href="#cb3-301"></a>    <span class="kw">def</span> estimate_mfu(<span class="va">self</span>, fwdbwd_per_iter, dt):</span>
<span id="cb3-302"><a href="#cb3-302"></a>        <span class="co">""" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS """</span></span>
<span id="cb3-303"><a href="#cb3-303"></a>        <span class="co"># first estimate the number of flops we do per iteration.</span></span>
<span id="cb3-304"><a href="#cb3-304"></a>        <span class="co"># see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311</span></span>
<span id="cb3-305"><a href="#cb3-305"></a>        N <span class="op">=</span> <span class="va">self</span>.get_num_params()</span>
<span id="cb3-306"><a href="#cb3-306"></a>        cfg <span class="op">=</span> <span class="va">self</span>.config</span>
<span id="cb3-307"><a href="#cb3-307"></a>        L, H, Q, T <span class="op">=</span> cfg.n_layer, cfg.n_head, cfg.n_embd<span class="op">//</span>cfg.n_head, cfg.block_size</span>
<span id="cb3-308"><a href="#cb3-308"></a>        flops_per_token <span class="op">=</span> <span class="dv">6</span><span class="op">*</span>N <span class="op">+</span> <span class="dv">12</span><span class="op">*</span>L<span class="op">*</span>H<span class="op">*</span>Q<span class="op">*</span>T</span>
<span id="cb3-309"><a href="#cb3-309"></a>        flops_per_fwdbwd <span class="op">=</span> flops_per_token <span class="op">*</span> T</span>
<span id="cb3-310"><a href="#cb3-310"></a>        flops_per_iter <span class="op">=</span> flops_per_fwdbwd <span class="op">*</span> fwdbwd_per_iter</span>
<span id="cb3-311"><a href="#cb3-311"></a>        <span class="co"># express our flops throughput as ratio of A100 bfloat16 peak flops</span></span>
<span id="cb3-312"><a href="#cb3-312"></a>        flops_achieved <span class="op">=</span> flops_per_iter <span class="op">*</span> (<span class="fl">1.0</span><span class="op">/</span>dt) <span class="co"># per second</span></span>
<span id="cb3-313"><a href="#cb3-313"></a>        flops_promised <span class="op">=</span> <span class="fl">312e12</span> <span class="co"># A100 GPU bfloat16 peak flops is 312 TFLOPS</span></span>
<span id="cb3-314"><a href="#cb3-314"></a>        mfu <span class="op">=</span> flops_achieved <span class="op">/</span> flops_promised</span>
<span id="cb3-315"><a href="#cb3-315"></a>        <span class="cf">return</span> mfu</span>
<span id="cb3-316"><a href="#cb3-316"></a></span>
<span id="cb3-317"><a href="#cb3-317"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb3-318"><a href="#cb3-318"></a>    <span class="kw">def</span> generate(<span class="va">self</span>, idx, max_new_tokens, temperature<span class="op">=</span><span class="fl">1.0</span>, top_k<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-319"><a href="#cb3-319"></a>        <span class="co">"""</span></span>
<span id="cb3-320"><a href="#cb3-320"></a><span class="co">        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete</span></span>
<span id="cb3-321"><a href="#cb3-321"></a><span class="co">        the sequence max_new_tokens times, feeding the predictions back into the model each time.</span></span>
<span id="cb3-322"><a href="#cb3-322"></a><span class="co">        Most likely you'll want to make sure to be in model.eval() mode of operation for this.</span></span>
<span id="cb3-323"><a href="#cb3-323"></a><span class="co">        """</span></span>
<span id="cb3-324"><a href="#cb3-324"></a>        <span class="cf">for</span> _ <span class="kw">in</span> <span class="bu">range</span>(max_new_tokens):</span>
<span id="cb3-325"><a href="#cb3-325"></a>            <span class="co"># if the sequence context is growing too long we must crop it at block_size</span></span>
<span id="cb3-326"><a href="#cb3-326"></a>            idx_cond <span class="op">=</span> idx <span class="cf">if</span> idx.size(<span class="dv">1</span>) <span class="op">&lt;=</span> <span class="va">self</span>.config.block_size <span class="cf">else</span> idx[:, <span class="op">-</span><span class="va">self</span>.config.block_size:]</span>
<span id="cb3-327"><a href="#cb3-327"></a>            <span class="co"># forward the model to get the logits for the index in the sequence</span></span>
<span id="cb3-328"><a href="#cb3-328"></a>            logits, _ <span class="op">=</span> <span class="va">self</span>(idx_cond)</span>
<span id="cb3-329"><a href="#cb3-329"></a>            <span class="co"># pluck the logits at the final step and scale by desired temperature</span></span>
<span id="cb3-330"><a href="#cb3-330"></a>            logits <span class="op">=</span> logits[:, <span class="op">-</span><span class="dv">1</span>, :] <span class="op">/</span> temperature</span>
<span id="cb3-331"><a href="#cb3-331"></a>            <span class="co"># optionally crop the logits to only the top k options</span></span>
<span id="cb3-332"><a href="#cb3-332"></a>            <span class="cf">if</span> top_k <span class="kw">is</span> <span class="kw">not</span> <span class="va">None</span>:</span>
<span id="cb3-333"><a href="#cb3-333"></a>                v, _ <span class="op">=</span> torch.topk(logits, <span class="bu">min</span>(top_k, logits.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb3-334"><a href="#cb3-334"></a>                logits[logits <span class="op">&lt;</span> v[:, [<span class="op">-</span><span class="dv">1</span>]]] <span class="op">=</span> <span class="op">-</span><span class="bu">float</span>(<span class="st">'Inf'</span>)</span>
<span id="cb3-335"><a href="#cb3-335"></a>            <span class="co"># apply softmax to convert logits to (normalized) probabilities</span></span>
<span id="cb3-336"><a href="#cb3-336"></a>            probs <span class="op">=</span> F.softmax(logits, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb3-337"><a href="#cb3-337"></a>            <span class="co"># sample from the distribution</span></span>
<span id="cb3-338"><a href="#cb3-338"></a>            idx_next <span class="op">=</span> torch.multinomial(probs, num_samples<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-339"><a href="#cb3-339"></a>            <span class="co"># append sampled index to the running sequence and continue</span></span>
<span id="cb3-340"><a href="#cb3-340"></a>            idx <span class="op">=</span> torch.cat((idx, idx_next), dim<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb3-341"><a href="#cb3-341"></a></span>
<span id="cb3-342"><a href="#cb3-342"></a>        <span class="cf">return</span> idx</span></code></pre></div>
</div>
<div id="tabset-1-2">
<div class="sourceCode" id="cb4" data-code-line-numbers="true" data-startfrom="32"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 31;"><span id="cb4-32"><a href="#cb4-32"></a><span class="kw">class</span> LayerNorm(nn.Module):</span>
<span id="cb4-33"><a href="#cb4-33"></a>    <span class="co">""" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False """</span></span>
<span id="cb4-34"><a href="#cb4-34"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, ndim, bias):</span>
<span id="cb4-35"><a href="#cb4-35"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb4-36"><a href="#cb4-36"></a>        <span class="va">self</span>.weight <span class="op">=</span> nn.Parameter(torch.ones(ndim))</span>
<span id="cb4-37"><a href="#cb4-37"></a>        <span class="va">self</span>.bias <span class="op">=</span> nn.Parameter(torch.zeros(ndim)) <span class="cf">if</span> bias <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb4-38"><a href="#cb4-38"></a></span>
<span id="cb4-39"><a href="#cb4-39"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, <span class="bu">input</span>):</span>
<span id="cb4-40"><a href="#cb4-40"></a>        <span class="cf">return</span> F.layer_norm(<span class="bu">input</span>, <span class="va">self</span>.weight.shape, <span class="va">self</span>.weight, <span class="va">self</span>.bias, <span class="fl">1e-5</span>)</span></code></pre></div>
</div>
<div id="tabset-1-3">
<div class="sourceCode" id="cb5" data-code-line-numbers="true" data-startfrom="43"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 42;"><span id="cb5-43"><a href="#cb5-43"></a><span class="kw">class</span> CausalSelfAttention(nn.Module):</span>
<span id="cb5-44"><a href="#cb5-44"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb5-45"><a href="#cb5-45"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb5-46"><a href="#cb5-46"></a>        <span class="cf">assert</span> config.n_embd <span class="op">%</span> config.n_head <span class="op">==</span> <span class="dv">0</span></span>
<span id="cb5-47"><a href="#cb5-47"></a>        <span class="co"># key, query, value projections for all heads, but in a batch</span></span>
<span id="cb5-48"><a href="#cb5-48"></a>        <span class="va">self</span>.c_attn <span class="op">=</span> nn.Linear(config.n_embd, <span class="dv">3</span> <span class="op">*</span> config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb5-49"><a href="#cb5-49"></a>        <span class="co"># output projection</span></span>
<span id="cb5-50"><a href="#cb5-50"></a>        <span class="va">self</span>.c_proj <span class="op">=</span> nn.Linear(config.n_embd, config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb5-51"><a href="#cb5-51"></a>        <span class="co"># regularization</span></span>
<span id="cb5-52"><a href="#cb5-52"></a>        <span class="va">self</span>.attn_dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb5-53"><a href="#cb5-53"></a>        <span class="va">self</span>.resid_dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb5-54"><a href="#cb5-54"></a>        <span class="va">self</span>.n_head <span class="op">=</span> config.n_head</span>
<span id="cb5-55"><a href="#cb5-55"></a>        <span class="va">self</span>.n_embd <span class="op">=</span> config.n_embd</span>
<span id="cb5-56"><a href="#cb5-56"></a>        <span class="va">self</span>.dropout <span class="op">=</span> config.dropout</span>
<span id="cb5-57"><a href="#cb5-57"></a>        <span class="co"># flash attention make GPU go brrrrr but support is only in PyTorch &gt;= 2.0</span></span>
<span id="cb5-58"><a href="#cb5-58"></a>        <span class="va">self</span>.flash <span class="op">=</span> <span class="bu">hasattr</span>(torch.nn.functional, <span class="st">'scaled_dot_product_attention'</span>)</span>
<span id="cb5-59"><a href="#cb5-59"></a>        <span class="co"># if self.flash and RANK == 0:</span></span>
<span id="cb5-60"><a href="#cb5-60"></a>        <span class="co">#     log.warning(f'Using torch.nn.functional.scaled_dot_product_attention (Flash Attn)')</span></span>
<span id="cb5-61"><a href="#cb5-61"></a>        <span class="cf">if</span> <span class="kw">not</span> <span class="va">self</span>.flash:</span>
<span id="cb5-62"><a href="#cb5-62"></a>            log.warning(<span class="st">"WARNING: using slow attention. Flash Attention requires PyTorch &gt;= 2.0"</span>)</span>
<span id="cb5-63"><a href="#cb5-63"></a>            <span class="co"># causal mask to ensure that attention is only applied to the left in the input sequence</span></span>
<span id="cb5-64"><a href="#cb5-64"></a>            <span class="va">self</span>.register_buffer(</span>
<span id="cb5-65"><a href="#cb5-65"></a>                <span class="st">"bias"</span>,</span>
<span id="cb5-66"><a href="#cb5-66"></a>                torch.tril(</span>
<span id="cb5-67"><a href="#cb5-67"></a>                    torch.ones(</span>
<span id="cb5-68"><a href="#cb5-68"></a>                        config.block_size,</span>
<span id="cb5-69"><a href="#cb5-69"></a>                        config.block_size</span>
<span id="cb5-70"><a href="#cb5-70"></a>                    )</span>
<span id="cb5-71"><a href="#cb5-71"></a>                ).view(<span class="dv">1</span>, <span class="dv">1</span>, config.block_size, config.block_size)</span>
<span id="cb5-72"><a href="#cb5-72"></a>            )</span>
<span id="cb5-73"><a href="#cb5-73"></a></span>
<span id="cb5-74"><a href="#cb5-74"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb5-75"><a href="#cb5-75"></a>        B, T, C <span class="op">=</span> x.size() <span class="co"># batch size, sequence length, embedding dimensionality (n_embd)</span></span>
<span id="cb5-76"><a href="#cb5-76"></a></span>
<span id="cb5-77"><a href="#cb5-77"></a>        <span class="co"># calculate query, key, values for all heads in batch and move head forward to be the batch dim</span></span>
<span id="cb5-78"><a href="#cb5-78"></a>        q, k, v  <span class="op">=</span> <span class="va">self</span>.c_attn(x).split(<span class="va">self</span>.n_embd, dim<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb5-79"><a href="#cb5-79"></a>        k <span class="op">=</span> k.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb5-80"><a href="#cb5-80"></a>        q <span class="op">=</span> q.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb5-81"><a href="#cb5-81"></a>        v <span class="op">=</span> v.view(B, T, <span class="va">self</span>.n_head, C <span class="op">//</span> <span class="va">self</span>.n_head).transpose(<span class="dv">1</span>, <span class="dv">2</span>) <span class="co"># (B, nh, T, hs)</span></span>
<span id="cb5-82"><a href="#cb5-82"></a></span>
<span id="cb5-83"><a href="#cb5-83"></a>        <span class="co"># causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -&gt; (B, nh, T, T)</span></span>
<span id="cb5-84"><a href="#cb5-84"></a>        <span class="cf">if</span> <span class="va">self</span>.flash:</span>
<span id="cb5-85"><a href="#cb5-85"></a>            <span class="co"># efficient attention using Flash Attention CUDA kernels</span></span>
<span id="cb5-86"><a href="#cb5-86"></a>            y <span class="op">=</span> torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask<span class="op">=</span><span class="va">None</span>, dropout_p<span class="op">=</span><span class="va">self</span>.dropout <span class="cf">if</span> <span class="va">self</span>.training <span class="cf">else</span> <span class="dv">0</span>, is_causal<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb5-87"><a href="#cb5-87"></a>        <span class="cf">else</span>:</span>
<span id="cb5-88"><a href="#cb5-88"></a>            <span class="co"># manual implementation of attention</span></span>
<span id="cb5-89"><a href="#cb5-89"></a>            att <span class="op">=</span> (q <span class="op">@</span> k.transpose(<span class="op">-</span><span class="dv">2</span>, <span class="op">-</span><span class="dv">1</span>)) <span class="op">*</span> (<span class="fl">1.0</span> <span class="op">/</span> math.sqrt(k.size(<span class="op">-</span><span class="dv">1</span>)))</span>
<span id="cb5-90"><a href="#cb5-90"></a>            att <span class="op">=</span> att.masked_fill(<span class="va">self</span>.bias[:,:,:T,:T] <span class="op">==</span> <span class="dv">0</span>, <span class="bu">float</span>(<span class="st">'-inf'</span>))</span>
<span id="cb5-91"><a href="#cb5-91"></a>            att <span class="op">=</span> F.softmax(att, dim<span class="op">=-</span><span class="dv">1</span>)</span>
<span id="cb5-92"><a href="#cb5-92"></a>            att <span class="op">=</span> <span class="va">self</span>.attn_dropout(att)</span>
<span id="cb5-93"><a href="#cb5-93"></a>            y <span class="op">=</span> att <span class="op">@</span> v <span class="co"># (B, nh, T, T) x (B, nh, T, hs) -&gt; (B, nh, T, hs)</span></span>
<span id="cb5-94"><a href="#cb5-94"></a>        y <span class="op">=</span> y.transpose(<span class="dv">1</span>, <span class="dv">2</span>).contiguous().view(B, T, C) <span class="co"># re-assemble all head outputs side by side</span></span>
<span id="cb5-95"><a href="#cb5-95"></a></span>
<span id="cb5-96"><a href="#cb5-96"></a>        <span class="co"># output projection</span></span>
<span id="cb5-97"><a href="#cb5-97"></a>        y <span class="op">=</span> <span class="va">self</span>.resid_dropout(<span class="va">self</span>.c_proj(y))</span>
<span id="cb5-98"><a href="#cb5-98"></a>        <span class="cf">return</span> y</span></code></pre></div>
</div>
<div id="tabset-1-4">
<div class="sourceCode" id="cb6" data-code-line-numbers="true" data-startfrom="100"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 99;"><span id="cb6-100"><a href="#cb6-100"></a><span class="kw">class</span> MLP(nn.Module):</span>
<span id="cb6-101"><a href="#cb6-101"></a></span>
<span id="cb6-102"><a href="#cb6-102"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb6-103"><a href="#cb6-103"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-104"><a href="#cb6-104"></a>        <span class="va">self</span>.c_fc    <span class="op">=</span> nn.Linear(config.n_embd, <span class="dv">4</span> <span class="op">*</span> config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb6-105"><a href="#cb6-105"></a>        <span class="va">self</span>.gelu    <span class="op">=</span> nn.GELU()</span>
<span id="cb6-106"><a href="#cb6-106"></a>        <span class="va">self</span>.c_proj  <span class="op">=</span> nn.Linear(<span class="dv">4</span> <span class="op">*</span> config.n_embd, config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb6-107"><a href="#cb6-107"></a>        <span class="va">self</span>.dropout <span class="op">=</span> nn.Dropout(config.dropout)</span>
<span id="cb6-108"><a href="#cb6-108"></a></span>
<span id="cb6-109"><a href="#cb6-109"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-110"><a href="#cb6-110"></a>        x <span class="op">=</span> <span class="va">self</span>.c_fc(x)</span>
<span id="cb6-111"><a href="#cb6-111"></a>        x <span class="op">=</span> <span class="va">self</span>.gelu(x)</span>
<span id="cb6-112"><a href="#cb6-112"></a>        x <span class="op">=</span> <span class="va">self</span>.c_proj(x)</span>
<span id="cb6-113"><a href="#cb6-113"></a>        x <span class="op">=</span> <span class="va">self</span>.dropout(x)</span>
<span id="cb6-114"><a href="#cb6-114"></a>        <span class="cf">return</span> x</span>
<span id="cb6-115"><a href="#cb6-115"></a></span>
<span id="cb6-116"><a href="#cb6-116"></a><span class="kw">class</span> Block(nn.Module):</span>
<span id="cb6-117"><a href="#cb6-117"></a></span>
<span id="cb6-118"><a href="#cb6-118"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, config: ModelConfig):</span>
<span id="cb6-119"><a href="#cb6-119"></a>        <span class="bu">super</span>().<span class="fu">__init__</span>()</span>
<span id="cb6-120"><a href="#cb6-120"></a>        <span class="va">self</span>.ln_1 <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb6-121"><a href="#cb6-121"></a>        <span class="va">self</span>.attn <span class="op">=</span> CausalSelfAttention(config)</span>
<span id="cb6-122"><a href="#cb6-122"></a>        <span class="va">self</span>.ln_2 <span class="op">=</span> LayerNorm(config.n_embd, bias<span class="op">=</span>config.bias)</span>
<span id="cb6-123"><a href="#cb6-123"></a>        <span class="va">self</span>.mlp <span class="op">=</span> MLP(config)</span>
<span id="cb6-124"><a href="#cb6-124"></a></span>
<span id="cb6-125"><a href="#cb6-125"></a>    <span class="kw">def</span> forward(<span class="va">self</span>, x):</span>
<span id="cb6-126"><a href="#cb6-126"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.attn(<span class="va">self</span>.ln_1(x))</span>
<span id="cb6-127"><a href="#cb6-127"></a>        x <span class="op">=</span> x <span class="op">+</span> <span class="va">self</span>.mlp(<span class="va">self</span>.ln_2(x))</span>
<span id="cb6-128"><a href="#cb6-128"></a>        <span class="cf">return</span> x</span></code></pre></div>
</div>
</div>
</div>
</section>

<section id="iconify-fa-brands-github-trainer.py" class="title-slide slide level1 center" height="100%">
<h1><a href="https://github.com/saforem2/nanoGPT/blob/master/src/ngpt/trainer.py"><iconify-icon inline="" icon="fa-brands:github"></iconify-icon> <code>trainer.py</code></a></h1>
<div class="panel-tabset" style="font-size: 0.75em; width: 100%!important; height: 100%!important;">
<ul id="tabset-2" class="panel-tabset-tabby"><li><a data-tabby-default="" href="#tabset-2-1"><code>get_batch</code></a></li><li><a href="#tabset-2-2"><code>estimate_loss</code></a></li><li><a href="#tabset-2-3"><code>_forward_step</code></a></li><li><a href="#tabset-2-4"><code>_backward_step</code></a></li><li><a href="#tabset-2-5"><code>train_step</code></a></li></ul>
<div class="tab-content" style="font-size: 0.75em; width: 100%!important; height: 100%!important;">
<div id="tabset-2-1">
<div class="sourceCode" id="cb7" data-code-line-numbers="true" data-startfrom="233"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 232;"><span id="cb7-233"><a href="#cb7-233"></a>    <span class="kw">def</span> get_batch(<span class="va">self</span>, split: <span class="bu">str</span>) <span class="op">-&gt;</span> <span class="bu">tuple</span>[torch.Tensor, torch.Tensor]:</span>
<span id="cb7-234"><a href="#cb7-234"></a>        data <span class="op">=</span> <span class="va">self</span>.config.train_data <span class="cf">if</span> split <span class="op">==</span> <span class="st">'train'</span> <span class="cf">else</span> <span class="va">self</span>.config.val_data</span>
<span id="cb7-235"><a href="#cb7-235"></a>        ix <span class="op">=</span> torch.randint(</span>
<span id="cb7-236"><a href="#cb7-236"></a>            <span class="bu">len</span>(data) <span class="op">-</span> <span class="va">self</span>.config.model.block_size,</span>
<span id="cb7-237"><a href="#cb7-237"></a>            (<span class="va">self</span>.config.model.batch_size,)</span>
<span id="cb7-238"><a href="#cb7-238"></a>        )</span>
<span id="cb7-239"><a href="#cb7-239"></a>        block_size <span class="op">=</span> <span class="va">self</span>.config.model.block_size</span>
<span id="cb7-240"><a href="#cb7-240"></a>        x <span class="op">=</span> torch.stack(</span>
<span id="cb7-241"><a href="#cb7-241"></a>            [</span>
<span id="cb7-242"><a href="#cb7-242"></a>                torch.from_numpy((data[i:i<span class="op">+</span>block_size]).astype(np.int64))</span>
<span id="cb7-243"><a href="#cb7-243"></a>                <span class="cf">for</span> i <span class="kw">in</span> ix</span>
<span id="cb7-244"><a href="#cb7-244"></a>            ]</span>
<span id="cb7-245"><a href="#cb7-245"></a>        )</span>
<span id="cb7-246"><a href="#cb7-246"></a>        y <span class="op">=</span> torch.stack(</span>
<span id="cb7-247"><a href="#cb7-247"></a>            [</span>
<span id="cb7-248"><a href="#cb7-248"></a>                torch.from_numpy((data[i<span class="op">+</span><span class="dv">1</span>:i<span class="op">+</span><span class="dv">1</span><span class="op">+</span>block_size]).astype(np.int64))</span>
<span id="cb7-249"><a href="#cb7-249"></a>                <span class="cf">for</span> i <span class="kw">in</span> ix</span>
<span id="cb7-250"><a href="#cb7-250"></a>            ]</span>
<span id="cb7-251"><a href="#cb7-251"></a>        )</span>
<span id="cb7-252"><a href="#cb7-252"></a>        <span class="cf">if</span> <span class="va">self</span>.config.device_type <span class="op">==</span> <span class="st">'cuda'</span>:</span>
<span id="cb7-253"><a href="#cb7-253"></a>            x <span class="op">=</span> x.pin_memory().to(<span class="va">self</span>.config.device_type, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-254"><a href="#cb7-254"></a>            y <span class="op">=</span> y.pin_memory().to(<span class="va">self</span>.config.device_type, non_blocking<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb7-255"><a href="#cb7-255"></a>        <span class="cf">else</span>:</span>
<span id="cb7-256"><a href="#cb7-256"></a>            x <span class="op">=</span> x.to(<span class="va">self</span>.config.device_type)</span>
<span id="cb7-257"><a href="#cb7-257"></a>            y <span class="op">=</span> y.to(<span class="va">self</span>.config.device_type)</span>
<span id="cb7-258"><a href="#cb7-258"></a>        <span class="cf">return</span> x, y</span></code></pre></div>
</div>
<div id="tabset-2-2">
<div class="sourceCode" id="cb8" data-code-line-numbers="true" data-startfrom="270"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 269;"><span id="cb8-270"><a href="#cb8-270"></a>    <span class="at">@torch.no_grad</span>()</span>
<span id="cb8-271"><a href="#cb8-271"></a>    <span class="kw">def</span> estimate_loss(<span class="va">self</span>):</span>
<span id="cb8-272"><a href="#cb8-272"></a>        out <span class="op">=</span> {}</span>
<span id="cb8-273"><a href="#cb8-273"></a>        <span class="va">self</span>.model.<span class="bu">eval</span>()</span>
<span id="cb8-274"><a href="#cb8-274"></a>        <span class="cf">for</span> split <span class="kw">in</span> [<span class="st">'train'</span>, <span class="st">'val'</span>]:</span>
<span id="cb8-275"><a href="#cb8-275"></a>            losses <span class="op">=</span> torch.zeros(<span class="va">self</span>.config.train.eval_iters)</span>
<span id="cb8-276"><a href="#cb8-276"></a>            <span class="cf">for</span> k <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.config.train.eval_iters):</span>
<span id="cb8-277"><a href="#cb8-277"></a>                x, y <span class="op">=</span> <span class="va">self</span>.get_batch(split)</span>
<span id="cb8-278"><a href="#cb8-278"></a>                <span class="cf">with</span> <span class="va">self</span>.config.ctx:</span>
<span id="cb8-279"><a href="#cb8-279"></a>                    _, loss <span class="op">=</span> <span class="va">self</span>.model(x, y)</span>
<span id="cb8-280"><a href="#cb8-280"></a>                losses[k] <span class="op">=</span> loss.item()</span>
<span id="cb8-281"><a href="#cb8-281"></a>            out[split] <span class="op">=</span> losses.mean()</span>
<span id="cb8-282"><a href="#cb8-282"></a>        <span class="va">self</span>.model.train()</span>
<span id="cb8-283"><a href="#cb8-283"></a>        <span class="cf">return</span> out</span></code></pre></div>
</div>
<div id="tabset-2-3">
<div class="sourceCode" id="cb9" data-code-line-numbers="true" data-startfrom="312"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 311;"><span id="cb9-312"><a href="#cb9-312"></a>    <span class="kw">def</span> _forward_step(<span class="va">self</span>, x: torch.Tensor, y: torch.Tensor) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb9-313"><a href="#cb9-313"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb9-314"><a href="#cb9-314"></a>        <span class="cf">with</span> <span class="va">self</span>.config.ctx:</span>
<span id="cb9-315"><a href="#cb9-315"></a>            logits, loss <span class="op">=</span> <span class="va">self</span>.model(x, y)</span>
<span id="cb9-316"><a href="#cb9-316"></a>        <span class="cf">return</span> {</span>
<span id="cb9-317"><a href="#cb9-317"></a>            <span class="st">'logits'</span>: logits,</span>
<span id="cb9-318"><a href="#cb9-318"></a>            <span class="st">'loss'</span>: loss,</span>
<span id="cb9-319"><a href="#cb9-319"></a>            <span class="st">'dt'</span>: time.perf_counter() <span class="op">-</span> t0</span>
<span id="cb9-320"><a href="#cb9-320"></a>        }</span></code></pre></div>
</div>
<div id="tabset-2-4">
<div class="sourceCode" id="cb10" data-code-line-numbers="true" data-startfrom="322"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 321;"><span id="cb10-322"><a href="#cb10-322"></a>    <span class="kw">def</span> _backward_step(</span>
<span id="cb10-323"><a href="#cb10-323"></a>            <span class="va">self</span>,</span>
<span id="cb10-324"><a href="#cb10-324"></a>            loss: torch.Tensor,</span>
<span id="cb10-325"><a href="#cb10-325"></a>            propagate_grads: <span class="bu">bool</span> <span class="op">=</span> <span class="va">False</span>,</span>
<span id="cb10-326"><a href="#cb10-326"></a>    ) <span class="op">-&gt;</span> <span class="bu">float</span>:</span>
<span id="cb10-327"><a href="#cb10-327"></a>        t0 <span class="op">=</span> time.perf_counter()</span>
<span id="cb10-328"><a href="#cb10-328"></a>        <span class="va">self</span>.scaler.scale(loss).backward()  <span class="co"># pyright: ignore</span></span>
<span id="cb10-329"><a href="#cb10-329"></a>        <span class="cf">if</span> propagate_grads:</span>
<span id="cb10-330"><a href="#cb10-330"></a>            <span class="cf">if</span> <span class="va">self</span>.config.optimizer.grad_clip <span class="op">!=</span> <span class="fl">0.0</span>:</span>
<span id="cb10-331"><a href="#cb10-331"></a>                <span class="va">self</span>.scaler.unscale_(<span class="va">self</span>.optimizer)</span>
<span id="cb10-332"><a href="#cb10-332"></a>                torch.nn.utils.clip_grad_norm_(  <span class="co"># pyright: ignore</span></span>
<span id="cb10-333"><a href="#cb10-333"></a>                    <span class="va">self</span>.model.parameters(),</span>
<span id="cb10-334"><a href="#cb10-334"></a>                    <span class="va">self</span>.config.optimizer.grad_clip</span>
<span id="cb10-335"><a href="#cb10-335"></a>                )</span>
<span id="cb10-336"><a href="#cb10-336"></a>            <span class="va">self</span>.scaler.step(<span class="va">self</span>.optimizer)</span>
<span id="cb10-337"><a href="#cb10-337"></a>            <span class="va">self</span>.scaler.update()</span>
<span id="cb10-338"><a href="#cb10-338"></a>            <span class="va">self</span>.optimizer.zero_grad(set_to_none<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb10-339"><a href="#cb10-339"></a></span>
<span id="cb10-340"><a href="#cb10-340"></a>        <span class="cf">return</span> time.perf_counter() <span class="op">-</span> t0</span></code></pre></div>
</div>
<div id="tabset-2-5">
<div class="sourceCode" id="cb11" data-code-line-numbers="true" data-startfrom="342"><pre class="sourceCode numberSource python number-lines"><code class="sourceCode python" style="counter-reset: source-line 341;"><span id="cb11-342"><a href="#cb11-342"></a>    <span class="kw">def</span> train_step(</span>
<span id="cb11-343"><a href="#cb11-343"></a>            <span class="va">self</span>,</span>
<span id="cb11-344"><a href="#cb11-344"></a>            x: torch.Tensor,</span>
<span id="cb11-345"><a href="#cb11-345"></a>            y: torch.Tensor,</span>
<span id="cb11-346"><a href="#cb11-346"></a>    ) <span class="op">-&gt;</span> <span class="bu">dict</span>:</span>
<span id="cb11-347"><a href="#cb11-347"></a>        lr <span class="op">=</span> (</span>
<span id="cb11-348"><a href="#cb11-348"></a>            <span class="va">self</span>.get_lr(<span class="va">self</span>.config.iter_num)</span>
<span id="cb11-349"><a href="#cb11-349"></a>            <span class="cf">if</span> <span class="va">self</span>.config.optimizer.decay_lr</span>
<span id="cb11-350"><a href="#cb11-350"></a>            <span class="cf">else</span> <span class="va">self</span>._lr</span>
<span id="cb11-351"><a href="#cb11-351"></a>        )</span>
<span id="cb11-352"><a href="#cb11-352"></a>        <span class="cf">for</span> param_group <span class="kw">in</span> <span class="va">self</span>.optimizer.param_groups:</span>
<span id="cb11-353"><a href="#cb11-353"></a>            param_group[<span class="st">'lr'</span>] <span class="op">=</span> lr</span>
<span id="cb11-354"><a href="#cb11-354"></a>        dtf <span class="op">=</span> []</span>
<span id="cb11-355"><a href="#cb11-355"></a>        dtb <span class="op">=</span> []</span>
<span id="cb11-356"><a href="#cb11-356"></a>        dt <span class="op">=</span> []</span>
<span id="cb11-357"><a href="#cb11-357"></a>        loss <span class="op">=</span> torch.tensor(<span class="fl">0.0</span>)</span>
<span id="cb11-358"><a href="#cb11-358"></a>        <span class="cf">for</span> micro_step <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>._gas):</span>
<span id="cb11-359"><a href="#cb11-359"></a>            is_last_micro_step <span class="op">=</span> (micro_step <span class="op">==</span> <span class="va">self</span>._gas <span class="op">-</span> <span class="dv">1</span>)</span>
<span id="cb11-360"><a href="#cb11-360"></a>            <span class="co"># </span><span class="al">NOTE</span><span class="co">: -----------------------------------------------------------</span></span>
<span id="cb11-361"><a href="#cb11-361"></a>            <span class="co"># In DDP training we only need to sync gradients at the last micro</span></span>
<span id="cb11-362"><a href="#cb11-362"></a>            <span class="co"># step. the official way to do this is with model.no_sync() context</span></span>
<span id="cb11-363"><a href="#cb11-363"></a>            <span class="co"># manager, but I really dislike that this bloats the code and</span></span>
<span id="cb11-364"><a href="#cb11-364"></a>            <span class="co"># forces us to repeat code looking at the source of that context</span></span>
<span id="cb11-365"><a href="#cb11-365"></a>            <span class="co"># manager, it just toggles this variable</span></span>
<span id="cb11-366"><a href="#cb11-366"></a>            <span class="co"># -----------------------------------------------------------------</span></span>
<span id="cb11-367"><a href="#cb11-367"></a>            _ <span class="op">=</span> (</span>
<span id="cb11-368"><a href="#cb11-368"></a>                <span class="va">self</span>.model.require_backward_grad_sync</span>
<span id="cb11-369"><a href="#cb11-369"></a>                <span class="cf">if</span> (is_last_micro_step <span class="kw">and</span> WORLD_SIZE <span class="op">&gt;</span> <span class="dv">1</span>)</span>
<span id="cb11-370"><a href="#cb11-370"></a>                <span class="cf">else</span> <span class="va">None</span></span>
<span id="cb11-371"><a href="#cb11-371"></a>            )</span>
<span id="cb11-372"><a href="#cb11-372"></a>            fout <span class="op">=</span> <span class="va">self</span>._forward_step(x, y)</span>
<span id="cb11-373"><a href="#cb11-373"></a>            <span class="co"># immediately async prefetch next batch while model is doing the forward pass on the GPU</span></span>
<span id="cb11-374"><a href="#cb11-374"></a>            x, y <span class="op">=</span> <span class="va">self</span>.get_batch(<span class="st">'train'</span>)</span>
<span id="cb11-375"><a href="#cb11-375"></a>            loss <span class="op">=</span> fout[<span class="st">'loss'</span>] <span class="op">/</span> <span class="va">self</span>._gas</span>
<span id="cb11-376"><a href="#cb11-376"></a>            dtf.append(fout[<span class="st">'dt'</span>])</span>
<span id="cb11-377"><a href="#cb11-377"></a>            dtb_ <span class="op">=</span> <span class="va">self</span>._backward_step(loss, propagate_grads<span class="op">=</span>is_last_micro_step)</span>
<span id="cb11-378"><a href="#cb11-378"></a>            dtb.append(dtb_)</span>
<span id="cb11-379"><a href="#cb11-379"></a>            dt.append(dtf <span class="op">+</span> dtb)</span>
<span id="cb11-380"><a href="#cb11-380"></a>        timers <span class="op">=</span> {</span>
<span id="cb11-381"><a href="#cb11-381"></a>            <span class="st">'iter'</span>: <span class="va">self</span>.config.iter_num,</span>
<span id="cb11-382"><a href="#cb11-382"></a>            <span class="st">'dt'</span>: np.array(dt),</span>
<span id="cb11-383"><a href="#cb11-383"></a>            <span class="st">'dt_tot'</span>: np.<span class="bu">sum</span>(dt),</span>
<span id="cb11-384"><a href="#cb11-384"></a>            <span class="st">'dt_avg'</span>: np.mean(dt),</span>
<span id="cb11-385"><a href="#cb11-385"></a>            <span class="st">'dtf'</span>: np.array(dtf),</span>
<span id="cb11-386"><a href="#cb11-386"></a>            <span class="st">'dtf_tot'</span>: np.<span class="bu">sum</span>(dtf),</span>
<span id="cb11-387"><a href="#cb11-387"></a>            <span class="st">'dtf_avg'</span>: np.mean(dtf),</span>
<span id="cb11-388"><a href="#cb11-388"></a>            <span class="st">'dtb'</span>: np.array(dtb),</span>
<span id="cb11-389"><a href="#cb11-389"></a>            <span class="st">'dtb_tot'</span>: np.<span class="bu">sum</span>(dtb),</span>
<span id="cb11-390"><a href="#cb11-390"></a>            <span class="st">'dtb_avg'</span>: np.mean(dtb)</span>
<span id="cb11-391"><a href="#cb11-391"></a>        }</span>
<span id="cb11-392"><a href="#cb11-392"></a>        metrics <span class="op">=</span> {</span>
<span id="cb11-393"><a href="#cb11-393"></a>            <span class="st">'iter'</span>: <span class="va">self</span>.config.iter_num,</span>
<span id="cb11-394"><a href="#cb11-394"></a>            <span class="st">'loss'</span>: loss,</span>
<span id="cb11-395"><a href="#cb11-395"></a>            <span class="st">'lr'</span>: lr,</span>
<span id="cb11-396"><a href="#cb11-396"></a>        }</span>
<span id="cb11-397"><a href="#cb11-397"></a>        <span class="va">self</span>.config.iter_num <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb11-398"><a href="#cb11-398"></a>        <span class="cf">return</span> {</span>
<span id="cb11-399"><a href="#cb11-399"></a>            <span class="st">'metrics'</span>: metrics,</span>
<span id="cb11-400"><a href="#cb11-400"></a>            <span class="st">'timers'</span>: timers,</span>
<span id="cb11-401"><a href="#cb11-401"></a>            <span class="st">'x'</span>: x,</span>
<span id="cb11-402"><a href="#cb11-402"></a>            <span class="st">'y'</span>: y,</span>
<span id="cb11-403"><a href="#cb11-403"></a>        }</span></code></pre></div>
</div>
</div>
</div>
</section>

<section id="section" class="title-slide slide level1 center" data-background-iframe="https://saforem2.github.io/nanoGPT">
<h1></h1>

</section>

<section id="links" class="title-slide slide level1 center">
<h1>Links</h1>
<ol type="1">
<li><a href="https://github.com/Hannibal046/Awesome-LLM/blob/main/README.md"><i class="fa-brands fa-github" aria-label="github"></i> Hannibal046/Awesome-LLM</a> <span class="inline-image"><a href="https://awesome.re"><img data-src="https://awesome.re/badge.svg" alt="Awesome"></a></span></li>
<li><a href="https://github.com/Mooler0410/LLMsPracticalGuide"><i class="fa-brands fa-github" aria-label="github"></i> Mooler0410/LLMsPracticalGuide</a></li>
<li><a href="https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g238b2698243_0_734https://docs.google.com/presentation/d/1636wKStYdT_yRPbJNrf8MLKpQghuWGDmyHinHhAKeXY/edit#slide=id.g238b2698243_0_734">Large Language Models (in 2023)</a></li>
<li><a href="http://jalammar.github.io/illustrated-transformer/">The Illustrated Transformer</a></li>
<li><a href="https://ig.ft.com/generative-ai/">Generative AI Exists because of the Transformer</a></li>
<li><a href="https://jaykmody.com/blog/gpt-from-scratch/">GPT in 60 Lines of Numpy</a></li>
<li><a href="https://openai.com/research/better-language-models">Better Language Models and their Implications</a><br>
</li>
<li><span class="green-text"><i class="fa-solid fa-flask-vial" aria-label="flask-vial"></i></span> <a href="https://bigscience.notion.site/ebe3760ae1724dcc92f2e6877de0938f?v=2faf85dc00794321be14bc892539dd4f">Progress / Artefacts / Outcomes from ðŸŒ¸ Bloom BigScience</a></li>
</ol>
<div class="callout callout-note callout-titled callout-style-default">
<div class="callout-body">
<div class="callout-title">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Acknowledgements</strong></p>
</div>
<div class="callout-content">
<p>This research used resources of the Argonne Leadership Computing Facility,<br>
which is a DOE Office of Science User Facility supported under Contract DE-AC02-06CH11357.</p>
</div>
</div>
</div>
</section>

<section id="references" class="title-slide slide level1 smaller scrollable">
<h1>References</h1>
<div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-yang2023harnessing" class="csl-entry" role="listitem">
Yang, Jingfeng, Hongye Jin, Ruixiang Tang, Xiaotian Han, Qizhang Feng, Haoming Jiang, Bing Yin, and Xia Hu. 2023. <span>â€œHarnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond.â€</span> <a href="https://arxiv.org/abs/2304.13712">https://arxiv.org/abs/2304.13712</a>.
</div>
<div id="ref-yao2023tree" class="csl-entry" role="listitem">
Yao, Shunyu, Dian Yu, Jeffrey Zhao, Izhak Shafran, Thomas L. Griffiths, Yuan Cao, and Karthik Narasimhan. 2023. <span>â€œTree of Thoughts: Deliberate Problem Solving with Large Language Models.â€</span> <a href="https://arxiv.org/abs/2305.10601">https://arxiv.org/abs/2305.10601</a>.
</div>
</div>


<img src="https://raw.githubusercontent.com/saforem2/llm-lunch-talk/main/docs/assets/anl.svg" class="slide-logo r-stretch"><div class="footer footer-default">

</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="site_libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="site_libs/revealjs/plugin/search/search.js"></script>
  <script src="site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":true,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true,"themes":[{"name":"Dark","theme":"css/dark.scss","highlightTheme":"css/syntax-dark.scss"},{"name":"Light","theme":"css/light.scss","highlightTheme":"css/syntax-light.scss"}]},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: false,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: true,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: false,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: true,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'light-border',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      const findCites = (el) => {
        const parentEl = el.parentElement;
        if (parentEl) {
          const cites = parentEl.dataset.cites;
          if (cites) {
            return {
              el,
              cites: cites.split(' ')
            };
          } else {
            return findCites(el.parentElement)
          }
        } else {
          return undefined;
        }
      };
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const citeInfo = findCites(ref);
        if (citeInfo) {
          tippyHover(citeInfo.el, function() {
            var popup = window.document.createElement('div');
            citeInfo.cites.forEach(function(cite) {
              var citeDiv = window.document.createElement('div');
              citeDiv.classList.add('hanging-indent');
              citeDiv.classList.add('csl-entry');
              var biblioDiv = window.document.getElementById('ref-' + cite);
              if (biblioDiv) {
                citeDiv.innerHTML = biblioDiv.innerHTML;
              }
              popup.appendChild(citeDiv);
            });
            return popup.innerHTML;
          });
        }
      }
    });
    </script>
    

</body></html>